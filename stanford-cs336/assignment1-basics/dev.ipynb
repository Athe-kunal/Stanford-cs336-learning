{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## UNDERSTANDING UNICODE\n",
    "\n",
    "1. `chr(0)` returns `'\\x00'` which is the null character\n",
    "\n",
    "2. `__repr__` renders chr(0) as a null character (empty string) in the stdio, but unicode renders into a string object with place holder characters\n",
    "\n",
    "3. When rendered in a string, it is represented as `x00`, but with print it renders in stdio, hence empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n"
     ]
    }
   ],
   "source": [
    "print(chr(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this text is \\x00'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"this text is \" + chr(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this text is \u0000\n"
     ]
    }
   ],
   "source": [
    "print(\"this text is \" + chr(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 13)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"this text is \" + chr(0)\n",
    "\n",
    "len(s), len(\"this text is \") #so it does add to the length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROBLEM 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are no out of distribution as each sequence of characters can be represented as list of integers\n",
    "\n",
    "1. UTF-8 is preferred because it is more byte-efficient and covers all the ASCII characters, emojis. Most of the dataset are stored in UTF-8, hence it is compatible to train too\n",
    "\n",
    "2. Some UTF-8 characters use multiple bytes, but as this is decoding one byte at a time, it is erroneous. It should decode all at once\n",
    "\n",
    "3. é decodes into two bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello there'\n"
     ]
    }
   ],
   "source": [
    "test_str = \"hello there\"\n",
    "s = test_str.encode(\"utf-8\")\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n"
     ]
    }
   ],
   "source": [
    "print(type(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello\\x00 there      '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str = \"hello\\x00 there      \"\n",
    "s = test_str.encode(\"utf-8\")\n",
    "\"\".join([bytes([k]).decode(\"utf-8\") for k in list(s)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hell\n"
     ]
    }
   ],
   "source": [
    "print(bytes([104, 101, 108, 108]).decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'h'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes([104,101])\n",
    "bytes([104])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc3 in position 0: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_utf8_bytests_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[38;5;28mbytes\u001b[39m([b]).decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mdecode_utf8_bytests_to_str_wrong\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\x00\u001b[39;49;00m\u001b[33;43mhelloé\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mdecode_utf8_bytests_to_str_wrong\u001b[39m\u001b[34m(bytestring)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_utf8_bytests_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xc3 in position 0: unexpected end of data"
     ]
    }
   ],
   "source": [
    "def decode_utf8_bytests_to_str_wrong(bytestring: bytes) -> str:\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "\n",
    "decode_utf8_bytests_to_str_wrong(\"\\x00helloé\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[226, 130, 172]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"€\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xe2'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes([226])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[195, 169]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list('é'.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROBLEM 3\n",
    "\n",
    "### NOTES\n",
    "\n",
    "* Our initial vocabulary size is 256\n",
    "\n",
    "* We pre-tokenize the dataset to break them into characters, else it will treat `dog` and `dog!` differently\n",
    "\n",
    "* If we don't pre-tokenize, then everytime we merge, we have to count the occurences again. If you pre-tokenize, and let's say you have three bytes [1,2,3]. Suppose [1,2] occur many times, and you merge them to 4. So now it is [4,3]. But you still know the count of 4, which is the total count of [1,2], it is additive\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "exclude = [\"<|endoftext|>\", \"<|startoftext|>\", \"<|pad|>\"]\n",
    "specials = \"|\".join(regex.escape(s) for s in exclude)\n",
    "\n",
    "PAT = rf\"\"\"\n",
    "   (?P<skip>{specials})(*SKIP)(*FAIL)    # 1) skip any of the specials\n",
    " | '(?:[sdmt]|ll|ve|re)                  # 2) contractions\n",
    " | [ ]?\\p{{L}}+                          # 3) letters, with optional leading space\n",
    " | [ ]?\\p{{N}}+                          # 4) numbers, with optional leading space\n",
    " | [ ]?[^\\s\\p{{L}}\\p{{N}}]+              # 5) other punctuation, opt. lead-space\n",
    " | \\s+(?!\\S)                             # 6) whitespace (but not trailing on a line)\n",
    " | \\s+                                   # 7) any whitespace\n",
    "\"\"\"\n",
    "\n",
    "token_re = regex.compile(PAT, regex.VERBOSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<regex.Match object; span=(0, 5), match='hello'>\n",
      "<regex.Match object; span=(5, 11), match=' there'>\n",
      "<regex.Match object; span=(11, 14), match=' <|'>\n",
      "<regex.Match object; span=(14, 23), match='endoftext'>\n",
      "<regex.Match object; span=(23, 25), match='|>'>\n"
     ]
    }
   ],
   "source": [
    "text = \"hello there <|endoftext|>\"\n",
    "for i in token_re.finditer(text):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ' ', 'there', ' ', '<|endoftext|>', ' ', 'friend', '!']\n"
     ]
    }
   ],
   "source": [
    "import regex\n",
    "\n",
    "# 1) List exactly the sequences you want to see as whole tokens:\n",
    "SPECIALS = [\"<|endoftext|>\", \"<|startoftext|>\", \"<|pad|>\"]\n",
    "special_pattern = \"|\".join(regex.escape(s) for s in SPECIALS)\n",
    "\n",
    "# 2) Build a master regex with the specials up front:\n",
    "PAT = rf\"\"\"\n",
    "   # — match any of your specials, first\n",
    "   {special_pattern}\n",
    "\n",
    "   | '(?:[sdmt]|ll|ve|re)   # contractions\n",
    "   | \\p{{L}}+               # letter sequences\n",
    "   | \\p{{N}}+               # number sequences\n",
    "   | [^\\s\\p{{L}}\\p{{N}}]+   # other punctuation\n",
    "   | \\s+                    # whitespace\n",
    "\"\"\"\n",
    "\n",
    "token_re = regex.compile(PAT, regex.VERBOSE)\n",
    "\n",
    "def tokenize(text):\n",
    "    return [m.group(0) for m in token_re.finditer(text)]\n",
    "\n",
    "# demo\n",
    "text = \"hello there <|endoftext|> friend!\"\n",
    "print(tokenize(text))\n",
    "# -> ['hello', ' ', 'there', ' ', '<|endoftext|>', ' ', 'friend', '!']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ' there', ' <|endoftext|>', ' friend', '!']\n"
     ]
    }
   ],
   "source": [
    "import regex   # 3rd-party regex module (needed for \\p{} Unicode props)\n",
    "\n",
    "SPECIALS = [\"<|endoftext|>\", \"<|startoftext|>\", \"<|pad|>\"]\n",
    "special_alt = \"|\".join(regex.escape(s) for s in SPECIALS)\n",
    "\n",
    "PAT = rf\"\"\"\n",
    "    [ ]?(?:{special_alt})              # 1) special tags, optional leading space\n",
    "  | '(?:[sdmt]|ll|ve|re)               # 2) contractions\n",
    "  | [ ]?\\p{{L}}+                       # 3) letters\n",
    "  | [ ]?\\p{{N}}+                       # 4) numbers\n",
    "  | [ ]?[^\\s\\p{{L}}\\p{{N}}]+           # 5) punctuation\n",
    "  | \\s+(?!\\S)                          # 6) trailing spaces at EOL\n",
    "  | \\s+                                # 7) other whitespace\n",
    "\"\"\"\n",
    "\n",
    "token_re = regex.compile(PAT, regex.VERBOSE)\n",
    "\n",
    "# quick demo\n",
    "text = \"hello there <|endoftext|> friend!\"\n",
    "print([m.group(0) for m in token_re.finditer(text)])\n",
    "# ['hello', ' ', 'there', ' ', '<|endoftext|>', ' ', 'friend', '!']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizer\n",
    "\n",
    "tok = tokenizer.Tokenizer.from_files(\n",
    "    \"/Users/athekunal/Desktop/Stanford-cs336/Stanford-cs336-learning/stanford-cs336/tinystories-val.json\",\n",
    "    special_tokens=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey there\n"
     ]
    }
   ],
   "source": [
    "text = \"hey there\"\n",
    "print(tok.decode(tok.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 101, 108, 108, 111, 32, 116, 104, 101, 114, 101]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello there'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.decode([72, 417, 108, 111, 261,272])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Hello theA'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes([72, 101, 108, 108, 111, 32, 116, 104, 101, 65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "d = 512\n",
    "\n",
    "theta_vals = 10_000**((-2 * torch.arange(0,d//2,1))/d)\n",
    "position = torch.arange(d)\n",
    "\n",
    "# cos_terms = torch.cos(theta_vals)\n",
    "\n",
    "m_theta = position*torch.repeat_interleave(theta_vals,repeats=2)\n",
    "\n",
    "m_theta_cos = torch.cos(m_theta)\n",
    "m_theta_sin = torch.sin(m_theta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_vals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
